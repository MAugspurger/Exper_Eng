{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"163sZlsdw3ffkMTF1ZlYQBanCKAeXwdLh","timestamp":1743774083556},{"file_id":"163sZlsdw3ffkMTF1ZlYQBanCKAeXwdLh","timestamp":1681334803348},{"file_id":"16JITiENZU-7uDzGgiu0lWSWzy3Y1Uugq","timestamp":1681187524827}],"mount_file_id":"1jBiba8SCuQIrpSgpahIkNFIhSWCVaUja","authorship_tag":"ABX9TyMHLz/HLUOAgJJwcrOvXH/N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Fast Fourier Transform\n","\n","This notebook does the following:\n","* imports a .wav audio file\n","* displays the signal in the time domain\n","* converts the signal into the frequency domain\n","* filters the signal in the frequency domain\n","* convert the signal back into the time domain and displays it. <br> <br>\n","\n","This is pretty neat ðŸ˜€  Hope you enjoy it! (And thanks to Dr. Van Howe for his help!)  <br><br>\n","\n","---\n","\n"],"metadata":{"id":"1ysb3_XkZWJn"}},{"cell_type":"markdown","source":["### Import the audio file\n","\n","The first couple cells import some needed libraries, and then import the audio file from your Google drive.  Change the `filename` in the third cell to define the folder from which you are importing.  This will take a couple minutes to run."],"metadata":{"id":"1ogWrjEzaVfO"}},{"cell_type":"code","source":["#import libraries needed\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.io import wavfile\n","from IPython.display import Audio\n","from scipy.fft import fft, ifft, fftshift,rfft,ifftshift"],"metadata":{"id":"bUVSvuB-x2Va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://github.com/MAugspurger/Exper_Eng/raw/main/Labs_and_Sensors/Sound/C_trumpet_E4.wav\n","rate, audio = wavfile.read('C_trumpet_E4.wav')\n"],"metadata":{"id":"WSa4yudG8EeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've imported a file that contains the sound of a trumpet playing a middle E.  Take a listen!"],"metadata":{"id":"uPDscawWGUje"}},{"cell_type":"code","source":["wavfile.write('audin.wav', rate, audio)\n","Audio('audin.wav')"],"metadata":{"id":"er5GsqTvGC39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that this audio file is a discretized wave signal: that is, it is an array of data that represents the amplitude of the wave at different moments in time.  In other words, it's just a one-dimensional list of numbers.  Here we can see the first 100 elements:"],"metadata":{"id":"AaYEMX2VIeG7"}},{"cell_type":"code","source":["print(audio[0:100])"],"metadata":{"id":"eED-AO1EG17Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Display the signal in the time domain\n","\n","Let's take a closer look at the discretized version of this signal.  To do so, we want to define and prints out some key information about the file:"],"metadata":{"id":"ItXEeBWlatv-"}},{"cell_type":"code","source":["# Define the length of the array\n","N = len(audio)\n","\n","# The sampling rate 'rate' is determined when the audio file\n","# is recorded; here that is converted into the time step size 'dt'\n","dt = 1/rate\n","\n","# Define the total time and the frequency resolution\n","total = N/rate\n","df = 1/total\n","\n","# This makes an index that goes from -half the pts to +half the pts in the audio file\n","# The center is 0 now, and this makes the math for the FFT work better\n","# This is then transformed into arrays for time and frequency\n","x = np.linspace(-N/2,N/2-1,N)\n","time = dt*x\n","freq = df*x\n","\n","# Print out the key values\n","print(\"N = \", N)\n","print(\"Rate = \", rate)\n","print(\"Dt = \",  dt)\n","print(\"Total = \",  total)\n","print(\"df = \",  df)"],"metadata":{"id":"H_aVGL07nIYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… âœ… Before going on, answer the \"Set 1\" questions on the worksheet."],"metadata":{"id":"L5Zy2Esx9sQo"}},{"cell_type":"code","source":["# plot the audio file as the waveform\n","plt.figure()\n","plt.plot(time, audio)\n","plt.xlabel('Time (s)')\n","plt.ylabel('Amplitude)');\n","# The next line changes the range of the x-axis\n","# Uncomment and change the arguments to zoom in on the signal\n","plt.xlim(0,0.02);"],"metadata":{"id":"mVMz5U5600or"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… âœ… Before going on, answer the \"Set 2\" questions on the worksheet."],"metadata":{"id":"7jSWcnJd_E1g"}},{"cell_type":"markdown","source":["### Convert the signal to the frequency domain\n","\n","In the time domain, the x-axis is in units of time.  The FFT transforms the signal--much like changing a coordinate system--so that the x-axis is in units of frequency.  This allows us to see which frequency sine waves are added together to get the complex signal that we started with."],"metadata":{"id":"1x8B8gWlcBQJ"}},{"cell_type":"code","source":["#  Now we'll do some Fourier magic!!!\n","yf=fftshift(fft(audio))\n","\n","# Plot the Fast Fourier Transform (FFT)\n","# Each spike represents a sine wave\n","plt.figure()\n","plt.plot(freq, abs(yf))\n","plt.xlabel('Frequency (Hz)')\n","plt.ylabel('Amplitude)')\n","# Change the line below to zoom in on relevant data\n","plt.xlim(0,1000);"],"metadata":{"id":"JhyuytajQG_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… âœ… Before going on, answer the \"Set 3\" questions on the worksheet."],"metadata":{"id":"amVBl_LpAmpf"}},{"cell_type":"markdown","source":["### Filtering the signal in the frequency domain\n","\n","Filtering in the frequency domain is pleasingly simply: all we need to do is multiply the frequencies that we want to keep by 1.0, and the frequencies that we want to filter by 0.   We'll set this up so that it can be a low-pass or a high pass filter. <br><br>\n","\n","You only need to change the variables in this first cell to control the filter:"],"metadata":{"id":"BQoVhrlNdaVi"}},{"cell_type":"code","source":["# Define whether you want a high pass or low pass filter.\n","# If you want a high pass filter, define 'hipass' as 1;\n","# for a low pass filter, define it as zero\n","hipass = 0\n","\n","# Define the cutoff frequency (in Hz)\n","cutoff = 500"],"metadata":{"id":"xIAdwepy4FIy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're going to make an array that includes only 1s and 0s that is the same size as the frequency domain audio file.   By multiplying that file element-wise with this array, we will keep some frequencies and eliminate others.  The image shows how element-wise multiplication works between two arrays:\n","\n","<img src = https://github.com/AugustanaPEA/ENGR_290/raw/main/Images/Element_wise_mult.PNG width = 300>"],"metadata":{"id":"8R0KZH2re0Qf"}},{"cell_type":"code","source":["# Convert the cutoff frequency from Hz to #pts in the array\n","edge = cutoff/df\n","\n","# Define the data points that are inside the filter\n","# There is a high and a low cutoff because the FFT produces\n","# positive and negative frequencies (that mirror each other)\n","range_lo=round(N/2-edge)\n","range_hi=round(N/2+edge)\n","\n","# Now we make our filter array, depending on the value of 'hipass'\n","if hipass == 1:\n","    yfilt=np.ones(len(freq))\n","    yfilt[range_lo:range_hi]=0\n","else:\n","    yfilt=np.zeros(len(freq))\n","    yfilt[range_lo:range_hi]=1"],"metadata":{"id":"2e22izJvevwk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we'll plot out the FFT with a red line showing us which frequencies will be kept and which will be filtered out:"],"metadata":{"id":"udbcJidxgTmW"}},{"cell_type":"code","source":["# Plot the FFT and the filter window\n","# to see if our filter is in the correct place\n","plt.figure()\n","plt.plot(freq, abs(yf))\n","plt.plot(freq,yfilt*max(abs(yf)),'-r')\n","plt.xlabel('Frequency (Hz)')\n","plt.ylabel('Amplitude)')\n","plt.xlim(0,2000);"],"metadata":{"id":"NoCWYHf4gSfP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we'll multiply the full frequency spectrum `yf` by the filter array in order to get a new filtered spectrum called `yfnew`.  The plot shows the spectrum of the filtered signal: we've removed the frequencies either above or below the cutoff frequency:"],"metadata":{"id":"kwin6Z3egf5W"}},{"cell_type":"code","source":["###time to do the filtering, just a simple multiplication in the freq domain\n","yfnew=yf*yfilt\n","\n","###plot filtered spectrum; should be just what we let in\n","plt.figure()\n","plt.plot(freq, abs(yfnew))\n","plt.plot(freq,yfilt*max(abs(yfnew)),'-r')\n","plt.xlabel('Frequency (kHz)')\n","plt.ylabel('Amplitude)')\n","plt.xlim(0,2000);"],"metadata":{"id":"JC5JcYQK4NML"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… âœ… Before going on, answer the \"Set 4\" questions on the worksheet."],"metadata":{"id":"9iHmkaY2KMGR"}},{"cell_type":"markdown","source":["### Convert the signal back to the time domain\n","\n","Finally, we want to see what the filtered signal looks like as a wave.  So we use the Inverse Fast Fourier Transform: this converts our frequency domain description of the filtered wave (that is, the plot just above this cell) into a time-domain description of the signal. <br><br>\n","\n","To be able to see what the filter did to the signal, the filtered signal is printed over the unfiltered signal.  Play with the filter parameters to see the effect of different filters!"],"metadata":{"id":"YFIWuYz9gtYe"}},{"cell_type":"code","source":["####transform back into time domain with inverse fft or ifft\n","yt=ifft(ifftshift(yfnew))\n","\n","\n","###Filtered wav in time domain\n","plt.figure()\n","plt.plot(time, np.real(yt),'-r')\n","plt.plot(time, audio, '--')\n","plt.xlabel('Time (s)')\n","plt.ylabel('Amplitude)')\n","plt.legend([\"Filtered Signal\", \"Unfiltered Signal\"], loc =\"upper right\")\n","# Play with the line below to zoom in or out\n","plt.xlim(0,0.02);"],"metadata":{"id":"cLhMuuOI49vV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can see the difference between the filtered and unfiltered signal, but you can also *hear* that difference if we convect the filtered signal back into an audio `.wav` file:"],"metadata":{"id":"Fc7djGNjLzCj"}},{"cell_type":"code","source":["# 'yt' is actually an array of complex numbers; we just want the real part\n","audiout=np.real(yt)\n","\n","# Now we can convert the file back into a .wav file and play it\n","audiout=np.asarray(audiout, dtype=np.int16)\n","wavfile.write('audiout.wav', rate, audiout)\n","Audio('audiout.wav')"],"metadata":{"id":"9-F0gRGpihRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now compare it to the original unfiltered signal\n","Audio('audin.wav')"],"metadata":{"id":"BTHDx5ZpNAGL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… âœ… Now play with the filter a bit, and answer \"Set 5\"."],"metadata":{"id":"T1V757kdNUTN"}},{"cell_type":"markdown","source":["### Use a filter and fourier analysis on your own voice\n","\n","Now we'll try the above process on a sound signal that you create.  To do this, you'll need to use an app called DecibelX.  **Note that this last step is here mostly because it's really interesting to see a voice represented as a sound signal.  But if you get stuck on a technical point in recording or uploading your clip, you do not need to spend a lot of time debugging: I will consider the assignment complete if you finish through step 5.\n","\n","<br>\n","\n","Follow these steps:\n","\n","1. Go to the App Store on your phone, and download the free DecibelX app.   You don't need to any advanced features, so no need to upgrade.\n","\n","2. You can play around with the app if you'd like.  Notice that it provides an instantaneous representation of sound in the frequency domain on the opening page (that is, frequency is on the x-axis).  You can slide the top screen (the plot) to the left, and see other representations of the sound.\n","\n","3. You want to record a clip of your voice singing or humming a constant note.  So first, clear the memory: press \"pause\", and then the circular arrow just next to the pause/play button.  Click \"OK\" to reset the recording data.\n","\n","4. Now you will record the clip.  Start singing/ humming a constant note.  Try to keep the amplitude and note as consistent as possible.  While singing, click \"play\" and keep singing for about 10 seconds.  While still singing, click \"pause\" and stop singing (if you'd like : )\n","\n","5. Now upload the clip to your computer.  Click the download arrow to the right of the \"play\" button, and click \"save\" (just keep the name \"Record 1\").  Then click the \"Data\" tab at the bottom of the screen.   You should see your \"Record 1\" listed on the Data page.   Click the lines-and-dots graphic in the upper right, which will allow you to choose a recording: check the circle that appeared next to \"Record 1\".\n","\n","6. Now click the \"upload\" arrow icon at the top of the screen.  Choose \"wav\" as the type of format.  Now choose to upload to Google Drive (or email if that is easier), and click \"Upload\".  Sign in as necessary.\n","\n","7. Now go to your Google Drive.  You should see a zipped Decibel X folder.  Open that, and \"extract all\" the files.  Move the \"Record 1\" file to the main folder of your Google Drive (that is, in \"My Drive\").\n","\n","8. Look at the file extension.  If it is in .WAV format, you are good: that is what we want, so no need to change this.  If you used an iPhone, however, the format will be .MP4 or .M4A.  In this case (or if the file is anything but a .WAV file), go to a converter website like https://cloudconvert.com/ and convert the file to a .WAV file, and download the converted file.  Move the downloaded file from your Downloads to your My Drive.  \n","\n","9. You should now have a .WAV file in the My Drive folder of your Google Drive.   For convenience, shorten the name of the file: \"record_1.wav\" would do fine.\n","\n","10. Now upload the file into Colab.  Check that the file name and location match the code below, and run these two cells:"],"metadata":{"id":"lI1Lu9qYMEj5"}},{"cell_type":"code","source":["# Grant Colab acces to your google drive\n","from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"id":"YgUae2kRxVJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read an audio wavfile\n","# Replace the file address here with the appropriate address\n","filename = '/gdrive/MyDrive/record_1.wav'\n","rate, audio_voice = wavfile.read(filename)"],"metadata":{"id":"bUtDA7jnoC4m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have to make some changes to the format of the .wav file because it differs a bit from the trumpet file we used above.  We'll name the new file \"audio\" so we don't have to change any more code above."],"metadata":{"id":"FNAOEwddQ07J"}},{"cell_type":"code","source":["n = len(audio_voice)\n","audio = np.zeros((n,), dtype=int)\n","for i in range(n):\n","    audio[i] = audio_voice[i][0]"],"metadata":{"id":"nFDHPdijIevX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Allow the file to be played in Colab\n","wavfile.write('audin.wav', rate, audio_voice)\n","Audio('audin.wav')"],"metadata":{"id":"_Civ4rGDDh5G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ok, now your voice file is ready to be analyzed and filtered!   Go back to the beginning of the notebook, and run the cells after the \"Display the signal in the time domain.\"  DO NOT \"Run All\", or you will overwrite the new \"audio\" file with the trumpet file."],"metadata":{"id":"8YamY8eHRxmg"}},{"cell_type":"markdown","source":["âœ… âœ… Now play with the filter on your voice file, looking at the form and sound of the filtered signal, and answer \"Set 6\"."],"metadata":{"id":"cOIr6-bpSZUP"}}]}